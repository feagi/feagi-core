use clap::Parser;
use log::info;
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;

/// FEAGI Inference Engine - Standalone neural processing engine with online learning
#[derive(Parser, Debug)]
#[command(name = "feagi-inference-engine", version, author, long_about = None)]
struct Args {
    /// Path to the connectome file to load
    #[arg(short, long)]
    connectome: PathBuf,

    /// Burst frequency in Hz (default: 50)
    #[arg(long, default_value_t = 50)]
    burst_hz: u64,

    /// Auto-save on shutdown
    #[arg(long, default_value_t = true)]
    auto_save: bool,

    /// Checkpoint interval in seconds (0 = disabled)
    #[arg(long, default_value_t = 0)]
    checkpoint_interval: u64,
}

/// Main entry point
fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logger
    env_logger::Builder::from_default_env()
        .filter_level(log::Level::Info.to_level_filter())
        .init();

    // Parse CLI arguments
    let args = Args::parse();

    // Banner
    print_banner();

    // Load connectome
    info!("Loading connectome from: {}", args.connectome.display());
    let connectome = feagi_connectome_serialization::load_connectome(&args.connectome)?;

    info!("âœ“ Connectome loaded successfully!");
    info!("  Neurons: {}/{}", connectome.neurons.count, connectome.neurons.capacity);
    info!("  Synapses: {}/{}", connectome.synapses.count, connectome.synapses.capacity);

    // Create NPU from connectome
    info!("Initializing NPU...");
    let npu = feagi_burst_engine::RustNPU::import_connectome(connectome);
    info!("âœ“ NPU initialized successfully!");

    // Setup signal handler
    let running = Arc::new(AtomicBool::new(true));
    let r = running.clone();
    ctrlc::set_handler(move || {
        info!("Shutdown signal received...");
        r.store(false, Ordering::SeqCst);
    })?;

    // Run engine
    info!("ðŸš€ Starting inference engine ({}Hz)", args.burst_hz);
    run_engine(npu, &args, running)?;

    info!("âœ… Inference engine shutdown complete!");
    Ok(())
}

/// Run the inference engine loop
fn run_engine(
    mut npu: feagi_burst_engine::RustNPU,
    args: &Args,
    running: Arc<AtomicBool>,
) -> Result<(), Box<dyn std::error::Error>> {
    let burst_interval = std::time::Duration::from_millis(1000 / args.burst_hz);
    let mut burst_count: u64 = 0;

    info!("ðŸ”„ Engine running (Press Ctrl+C to stop)...");

    while running.load(Ordering::Relaxed) {
        let start = std::time::Instant::now();

        // Execute burst (placeholder - needs actual neural processing)
        // TODO: Implement sensory input, burst execution, and motor output

        burst_count += 1;

        // Periodic status
        if burst_count % (args.burst_hz * 10) == 0 {
            info!("Status: {} bursts processed", burst_count);
        }

        // Checkpoint
        if args.checkpoint_interval > 0 && burst_count % (args.burst_hz * args.checkpoint_interval) == 0 {
            info!("Checkpoint at burst {}", burst_count);
            // TODO: Implement checkpointing
        }

        // Sleep to maintain frequency
        let elapsed = start.elapsed();
        if elapsed < burst_interval {
            std::thread::sleep(burst_interval - elapsed);
        }
    }

    info!("Stopped after {} bursts", burst_count);

    // Auto-save if enabled
    if args.auto_save {
        info!("Auto-saving connectome...");
        // TODO: Implement auto-save
        info!("âœ“ Connectome saved");
    }

    Ok(())
}

/// Print ASCII banner
fn print_banner() {
    println!(r#"
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                   â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—                           â•‘
â•‘   â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘                            â•‘
â•‘   â–ˆâ–ˆâ–ˆâ•”â•—  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘                           â•‘
â•‘   â–ˆâ–ˆâ•”â•â•  â–ˆâ–ˆâ•”  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•”â•â•â•â•   â–ˆâ–ˆâ•‘       â•‘
â•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘                 â–ˆâ–ˆâ•‘    â•‘â•‘
â•‘   â•šâ•â•   â•šâ•â•   â•šâ•â•â•â•â•â•â•   â•šâ•â•      â•šâ•â•        â•šâ•â•     â•šâ•â•â•â•â•â•â•   â•‘â•‘
â•‘                                                                   â•‘
â•‘   FEAGI Inference Engine v{}                                      â•‘
â•‘   Standalone Neural Processing System (FEAGI-TÂ®)                  â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "#, env!("CARGO_PKG_VERSION"));
}

